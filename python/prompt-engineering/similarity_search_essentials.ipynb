{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook Information\n",
    "\n",
    "This notebook demonstrates concept of similarity search and the difference between dense and sparse vectors.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "This notebook is maintained by:\n",
    "\n",
    "**Name:** Ekaterina Antonova\n",
    "**Email:** [ekaterina_antonova@epam.com](ekaterina_antonova@epam.com)\n",
    "\n",
    "**Name:** Adam Krzysiek \n",
    "**Email:** [adam_krzysiek@epam.com](adam_krzysiek@epam.com)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade pip > /dev/null\n",
    "%pip install langchain altair peft pandas umap-learn transformers numba langchain-huggingface FlagEmbedding > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import warnings\n",
    "import altair as alt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import umap.umap_ as umap\n",
    "from IPython.display import display\n",
    "from FlagEmbedding import BGEM3FlagModel\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from numba.core.errors import (\n",
    "    NumbaDeprecationWarning,\n",
    "    NumbaPendingDeprecationWarning,\n",
    ")\n",
    "from transformers import AutoModelForMaskedLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's start with some simple examples. First wee need to extract text and split it into chunks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,  # The maximum number of characters in a chunk: we selected this value arbitrarily\n",
    "    chunk_overlap=100,  # The number of characters to overlap between chunks\n",
    "    add_start_index=True,  # If `True`, includes chunk's start index in metadata\n",
    "    strip_whitespace=True,  # If `True`, strips whitespace from the start and end of every document\n",
    "    separators=[\"\\n\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ''\n",
    "with open('data/example_article.txt', 'r', encoding='utf-8') as file:\n",
    "    text += file.read()\n",
    "\n",
    "with open('data/example_article_2.txt', 'r', encoding='utf-8') as file:\n",
    "    text += file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = text_splitter.split_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, chunk in enumerate(chunks[:5]):\n",
    "    print(f\"Chunk {i + 1}:\")\n",
    "    print(chunk)\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dense Vectors\n",
    "Dense vectors are numerical representations (embeddings) generated by neural networks, specifically by text embedding models. These vectors usually have most or all of their elements as non-zero values.\n",
    "\n",
    "The main goal of dense vectors is to capture the semantic meaning of a piece of text. In simpler terms, they represent the essence of the text in a specific numerical format. The length of these vectors, called their \"dimension,\" varies depending on the model used (examples include dimensions like 256, 768, 1024, etc.).\n",
    "\n",
    "A single dimension in a dense vector embedding does not mean anything, as it is too abstract to determine its meaning. \n",
    "However, when we take all the dimensions together, they provide the semantic meaning of the input text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are interested to dive deeper into __representation of words__ and etc . Have a look at this wonderful __[presentation](https://nlp.cs.princeton.edu/cos484-sp21/lectures/lec5.pdf)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.simplefilter(\"ignore\", category=NumbaDeprecationWarning)\n",
    "warnings.simplefilter(\"ignore\", category=NumbaPendingDeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "encode_kwargs = {'normalize_embeddings': False}\n",
    "embedding_function = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_chunks = embedding_function.embed_documents(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization\n",
    "Now for the purpose of the visualization(we need to be able to build 2D plot), we need to reduce the dimentionality in our vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_chunks_np = np.array(embedded_chunks)\n",
    "\n",
    "# Reduce dimensionality with UMAP\n",
    "umap_model = umap.UMAP(n_components=2, random_state=42)\n",
    "umap_embeddings = umap_model.fit_transform(embedded_chunks_np)\n",
    "\n",
    "# Create a DataFrame for visualization\n",
    "df = pd.DataFrame({\n",
    "    'x': umap_embeddings[:, 0],\n",
    "    'y': umap_embeddings[:, 1],\n",
    "    'text': chunks\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that on the plot we have visualized like two different cluster one related to pets and domestic animals, \n",
    "the other for wild animals. It shows us that our embeddings capture this key differences and construct its vector space accordingly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that on the plot we have visualized like two different cluster one related to pets and domestic animals, \n",
    "the other for wild animals. It shows us that our embeddings capture this key differences and construct its vector space accordigly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also if you are interested in __UMAP__ as a technique, Higly reccomend this __[article](https://pair-code.github.io/understanding-umap/)__.\n",
    "It is worth to have a look at least for cool visualizations.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an interactive scatter plot with Altair\n",
    "chart = alt.Chart(df).mark_circle().encode(\n",
    "    x='x',\n",
    "    y='y',\n",
    "    tooltip=['text']\n",
    ").interactive()\n",
    "\n",
    "display(chart)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparse vectors\n",
    "Sparse Vectors are a representation where each dimension corresponds to a word or subword, greatly aiding in interpreting document rankings. This clarity is why sparse vectors are essential in modern search and recommendation systems, complimenting the meaning-rich embedding or dense vectors.\n",
    "\n",
    "Sparse Vectors shine in domains and scenarios where many rare keywords or specialized terms are present. For example, in the medical domain, many rare terms are not present in the general vocabulary, so general-purpose dense vectors cannot capture the nuances of the domain.\n",
    "\n",
    "Where do sparse vectors fail though? They’re not great at capturing nuanced relationships between words. For example, they can’t capture the relationship between “king” and “queen” as well as dense vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sparse embeddings are generated from algorithms like [BM25](https://en.wikipedia.org/wiki/Okapi_BM25) or with models like [BGE-M3](https://huggingface.co/BAAI/bge-m3) and  [SPLADE](https://arxiv.org/abs/2107.05720). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In most databases and search engines, such as Pinecone, Elasticsearch, and Milvus, there is a built-in implementation of BM25. Therefore, there is usually no need to implement it yourself; you can simply use the predefined capabilities they offer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create sparse embeddings using model __BGE-M3__, sometimes called lexical weights. This model actually is able to return sparse vectors and dense vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "text_example = chunks[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BGEM3FlagModel(\"BAAI/bge-m3\", use_fp16=True)\n",
    "sentences = [text_example]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model.encode(sentences, return_dense=False, return_sparse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_weight_id = output['lexical_weights']\n",
    "sparse_weight_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.convert_id_to_token(sparse_weight_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create sparse vectors using SPLADE\n",
    "model_id = 'naver/splade-cocondenser-ensembledistil'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_vector(text, tokenizer, model):\n",
    "    \"\"\"\n",
    "    Computes a vector from logits and attention mask using ReLU, log, and max operations.\n",
    "\n",
    "    Args:\n",
    "    logits (torch.Tensor): The logits output from a model.\n",
    "    attention_mask (torch.Tensor): The attention mask corresponding to the input tokens.\n",
    "\n",
    "    Returns:\n",
    "    torch.Tensor: Computed vector.\n",
    "    \"\"\"\n",
    "    tokens = tokenizer(text, return_tensors=\"pt\")\n",
    "    output = model(**tokens)\n",
    "    logits, attention_mask = output.logits, tokens.attention_mask\n",
    "    relu_log = torch.log(1 + torch.relu(logits))\n",
    "    weighted_log = relu_log * attention_mask.unsqueeze(-1)\n",
    "    max_val, _ = torch.max(weighted_log, dim=1)\n",
    "    vec = max_val.squeeze()\n",
    "\n",
    "    return vec, tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_and_map_sparse_vector(vector, tokenizer):\n",
    "    \"\"\"\n",
    "    Extracts non-zero elements from a given vector and maps these elements to their human-readable tokens using a tokenizer. The function creates and returns a sorted dictionary where keys are the tokens corresponding to non-zero elements in the vector, and values are the weights of these elements, sorted in descending order of weights.\n",
    "\n",
    "    This function is useful in NLP tasks where you need to understand the significance of different tokens based on a model's output vector. It first identifies non-zero values in the vector, maps them to tokens, and sorts them by weight for better interpretability.\n",
    "\n",
    "    Args:\n",
    "    vector (torch.Tensor): A PyTorch tensor from which to extract non-zero elements.\n",
    "    tokenizer: The tokenizer used for tokenization in the model, providing the mapping from tokens to indices.\n",
    "\n",
    "    Returns:\n",
    "    dict: A sorted dictionary mapping human-readable tokens to their corresponding non-zero weights.\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract indices and values of non-zero elements in the vector\n",
    "    cols = vector.nonzero().squeeze().cpu().tolist()\n",
    "    weights = vector[cols].cpu().tolist()\n",
    "\n",
    "    # Map indices to tokens and create a dictionary\n",
    "    idx2token = {idx: token for token, idx in tokenizer.get_vocab().items()}\n",
    "    token_weight_dict = {idx2token[idx]: round(weight, 2) for idx, weight in zip(cols, weights)}\n",
    "\n",
    "    # Sort the dictionary by weights in descending order\n",
    "    sorted_token_weight_dict = {k: v for k, v in sorted(token_weight_dict.items(), key=lambda item: item[1], reverse=True)}\n",
    "\n",
    "    return sorted_token_weight_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec, tokens = compute_vector(text_example, tokenizer=tokenizer, model=model)\n",
    "\n",
    "# Usage example\n",
    "sorted_tokens = extract_and_map_sparse_vector(vec, tokenizer)\n",
    "len(sorted_tokens), sorted_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There will be 60 sorted tokens in total. This has expanded to include tokens that weren’t in the original text. \n",
    "Consider a query “solar energy advantages”. SPLADE might expand this to include terms like “renewable,” “sustainable,” and “photovoltaic,” which are contextually relevant but not explicitly mentioned. This process is called term expansion, and it’s a key component of SPLADE.\n",
    "\n",
    "SPLADE learns the query/document expansion to include other relevant terms. This is a crucial advantage over other sparse methods which include the exact word, but completely miss the contextually relevant ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Feature                 | Sparse Vectors                          | Dense Vectors                        |\n",
    "|-------------------------|------------------------------------------|--------------------------------------|\n",
    "| Data Representation     | Majority of elements are zero           | All elements are non-zero            |\n",
    "| Computational Efficiency| Generally higher, especially in operations involving zero elements | Lower, as operations are performed on all elements |\n",
    "| Information Density     | Less dense, focuses on key features     | Highly dense, capturing nuanced relationships |\n",
    "| Example Applications    | Text search, Hybrid search              | RAG, many general machine learning tasks |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional materials: \n",
    "For those who want to dive deeper:\n",
    "\n",
    "* [Understanding the differences between sparse and dense semantic vectors](https://www.adelean.com/en/blog/20240131_vectors_sparse_and_dense/)\n",
    "* [Problem Motivation: Sparse Overcomplete Word Vector Representations](https://ar5iv.org/abs/1506.02004?utm_source=qdrant&utm_medium=website&utm_campaign=sparse-vectors&utm_content=article&utm_term=sparse-vectors)\n",
    "* [SPLADE v2: Sparse Lexical and Expansion Model for Information Retrieval](https://ar5iv.org/abs/2109.10086?utm_source=qdrant&utm_medium=website&utm_campaign=sparse-vectors&utm_content=article&utm_term=sparse-vectors)\n",
    "* [SPLADE: Sparse Lexical and Expansion Model for First Stage Ranking](https://ar5iv.org/abs/2107.05720?utm_source=qdrant&utm_medium=website&utm_campaign=sparse-vectors&utm_content=article&utm_term=sparse-vectors)\n",
    "* [Late Interaction - ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction](https://ar5iv.org/abs/2112.01488?utm_source=qdrant&utm_medium=website&utm_campaign=sparse-vectors&utm_content=article&utm_term=sparse-vectors)\n",
    "* [SparseEmbed: Learning Sparse Lexical Representations with Contextual Embeddings for Retrieval](https://research.google/pubs/pub52289/?utm_source=qdrant&utm_medium=website&utm_campaign=sparse-vectors&utm_content=article&utm_term=sparse-vectors)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b32d87a63d8db7c85c2cdcad4f6cc8b712ca746d91e154ea0cd737b1ef7d67c6"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
