{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56deb465",
   "metadata": {},
   "source": [
    "# Notebook Information\n",
    "\n",
    "This notebook demonstrates how to implement simgle RAG with Cromo db.\n",
    "\n",
    "---\n",
    "\n",
    "This notebook is maintained by:\n",
    "\n",
    "**Name:** Ekaterina Antonova\n",
    "**Email:** [ekaterina_antonova@epam.com](ekaterina_antonova@epam.com)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0cf609-1415-4d72-b42b-4fd55d9d175f",
   "metadata": {},
   "source": [
    "When discussing Retrieval-Augmented Generation (RAG), it's crucial to understand that there are four fundamental components that form the foundation of a successful RAG system. While you can enhance your RAG by incorporating advanced techniques inside this components, the components itself remain consistent. Here's a breakdown of each key component:\n",
    "\n",
    "#### 1. Document Processing\n",
    "This step involves extracting information from your documents, which is one of the most important aspects of RAG. Remember the rule of thumb: \"Garbage in, garbage out.\" The quality of your RAG system heavily depends on how you process documents. This includes choosing strategies for extracting, cleaning, splitting, and chunking information. The better the document processing, the more reliable the output of your RAG will be.\n",
    "#### 2. Retrieval Step\n",
    "This is a critical step where documents are fetched from your data store, allowing the Large Language Model (LLM) to generate responses based on these documents. Choosing an effective retrieval strategy is crucial. Various methods can be employed, such as dense vectors, sparse vectors, full-text search, or hybrid search approaches. The goal is to retrieve the most relevant documents to enchance accuracy of the system. \n",
    "#### 3. Re-Rank Step (Optional)\n",
    "In scenarios that require more precise ranking of the top 'n' results, re-ranking techniques are essential. While this step is optional, it's particularly useful for systems that need a higher degree of accuracy in the initial results. More advanced concepts and techniques for re-ranking will be covered in further topics.\n",
    "#### 4. Result Representation\n",
    "This component focuses on how the final output is presented to the user. It might involve summarization, chat capabilities, or other forms of analysis, often facilitated by an LLM. The goal here is to deliver the processed and retrieved information in a format that meets the user's needs effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f5beaa-9094-48f1-8f5c-6aef898734a8",
   "metadata": {},
   "source": [
    "### Let's explore these concepts in examples. We will do simple and vanila RAG with the help of langchain and chromo db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c6eb6a-1d07-45b7-9e24-1e5309b9192a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install openai sentence_transformers chromadb pypdf tiktoken langchain langchain_community langchain_core pydantic > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3af89c-be39-4d01-97ba-c87cfaa0b513",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dotenv\n",
    "import openai\n",
    "import time\n",
    "import tiktoken\n",
    "from openai import AzureOpenAI\n",
    "import json \n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings.base import Embeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from typing import List\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_openai import AzureChatOpenAI, AzureOpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d166a7e-fc85-4adc-94b2-764622b6f63a",
   "metadata": {},
   "source": [
    "### 1. Document processing\n",
    "#### Let's start with the file.  For our example we will take pdf form life science domain. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a612aa0-442e-4d3e-8a55-4ac0b93aeb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_pdf = \"https://www.delstrigo.com/wp-content/uploads/sites/32/2023/09/DELSTRIGO-Patient-Education-Brochure-PDF.pdf\"\n",
    "\n",
    "# Create your PDF loader\n",
    "loader = PyPDFLoader(url_pdf)\n",
    "\n",
    "# Load the PDF document\n",
    "documents = loader.load()\n",
    "\n",
    "# Chunk the financial report\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=0)\n",
    "docs = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7b4998-8405-4af2-b6d9-97f69adf568e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total text chunks: \", len(docs))\n",
    "print(\"\\033[1mContent of chunk:\\033[0m \\n\", docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc4b90f-df20-4dfd-868f-d0160527a400",
   "metadata": {},
   "source": [
    "#### We showed\n",
    "the basics of text extraction and splitting, specifically using tools readily available in the LangChain library. For a simple implementation, a vanilla text extractor and splitter was employed. However, there are more advanced techniques for text splitting, which will be covered in future lectures. \n",
    "\n",
    "For now, let's focus on a few key considerations when splitting text into chunks.\n",
    "#### Setting the chunk_size Parameter\n",
    "\n",
    "An important parameter in text splitting is chunk_size. In our example, we set it to 1024, but this value can vary based on several factors, such as:\n",
    "\n",
    "* __Model's Context Length__: The maximum input size a model can handle.\n",
    "* __Semantic Meaning__: The need to preserve meaningful segments within each chunk.\n",
    "* __Model's Ability to Retain Context__: Some models can manage larger contexts better than others.\n",
    " \n",
    "Choosing an appropriate chunk_size ensures that the model can effectively process and retain the information within each chunk.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ebcb3c0-fad7-4125-b7a7-864dfc63a625",
   "metadata": {},
   "source": [
    "### NB \n",
    "When using __RecursiveCharacterTextSplitter__, the chunk size is measured in characters. However, keep in mind that most models has their context window size in terms of __tokens__, not characters.\n",
    "\n",
    "__Tokens__: These are smaller units of text, such as words or subwords. The process of breaking down text into tokens is known as tokenization.\n",
    "\n",
    "### Measuring Tokens in Text\n",
    "\n",
    "To determine how many tokens are in a given piece of text, you can use the tokenizer associated with the model. For OpenAI's models, the tokenizer is called tiktoken. Here's a simple example of how to use tiktoken to count tokens in your text. \n",
    "If you want to read more about [tokenization](https://www.analyticsvidhya.com/blog/2020/05/what-is-tokenization-nlp/) . Also nice [OpenAI cookbook](https://cookbook.openai.com/examples/how_to_count_tokens_with_tiktoken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93a42ca-a9c2-4406-b766-8003abce67b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "content = docs[0].page_content\n",
    "\n",
    "# Simple length in charachters \n",
    "print(\"Characters: \", len(content))\n",
    "\n",
    "#Count length in tokens\n",
    "enc = tiktoken.encoding_for_model(\"gpt-4o\")\n",
    "print(\"Tokens: \", len(enc.encode(content)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42a7f8d-a9fd-4b2e-a0c0-66b43b299bff",
   "metadata": {},
   "source": [
    "### 2. Retrieval Step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172d8a4f-a822-4d39-9329-b6b34c3fb1b2",
   "metadata": {},
   "source": [
    "In this step, we'll explore examples using dense vectors and a hybrid approach. \n",
    "\n",
    "Full-Text Search (BM-25): Typically used as the baseline for many retrieval systems. It focuses on matching terms in the query with terms in the documents. For similarity search (RAG Baseline) it can be used as a baseline as well but in RAG, the standard approach is similarity search.  It involves embedding both the text and the query into dense vectors (also called embeddings) using a model. The retrieval process then calculates the distance between these vectors to rank the most relevant documents.\n",
    "\n",
    "#### Understanding Similarity Search\n",
    "\n",
    "__Distance Metrics__: To rank the vectors, you measure the \"distance\" between the document vectors and the query vector. Common metrics include:\n",
    " * Cosine Similarity: Measures the cosine of the angle between two vectors.\n",
    " * L2 (Euclidean) Distance: Measures the straight-line distance between vectors.\n",
    " * Inner Product: Calculates the dot product between vectors.\n",
    "Choosing the appropriate distance metric is crucial and often depends on the specific use case and the capabilities of the vector store being used.\n",
    "\n",
    "__Embedding__: Convert both your documents and the query into dense vectors using an embedding model.\n",
    "\n",
    "__Choosing a Model for Dense Vector Retrieval__\n",
    "\n",
    "Once you have decided on the distance metric, you also need to select an embedding model for dense vector retrieval. When choosing one, consider:\n",
    " * Context Length: The maximum input size the model can handle.\n",
    " * Model Size: Larger models might be more powerful but can be more computationally expensive.\n",
    " * Cost: Commercial models often have usage fees.\n",
    " * Domain Specificity: Some models are trained on specific domains, which may make them more effective for certain areas.\n",
    "\n",
    "Many large language model (LLM) providers, like OpenAI, offer their own embeddings. \n",
    "Open-Source Models: Numerous open-source models are available for embedding text. So no need to limit yourself with providers like OpenAI. \n",
    "\n",
    "To identify top-performing models, you can refer to the [MTEB (Massive Text Embedding Benchmark)](https://huggingface.co/spaces/mteb/leaderboard), which provides rankings and evaluations of various embedding models. \n",
    "\n",
    "More complex retrieval approaches you will learn in further lessons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acfad518-f120-433a-832f-b7086e6aaf00",
   "metadata": {},
   "outputs": [],
   "source": [
    "## How to make embedding using OpenAI API\n",
    "# Instantiate LLM\n",
    "\n",
    "azure_llm = AzureChatOpenAI(\n",
    "  api_key         = os.environ['OPENAI_API_KEY'],\n",
    "  api_version     = \"2023-07-01-preview\",\n",
    "  azure_endpoint  = \"https://ai-proxy.lab.epam.com\",\n",
    "  model           = \"gpt-4o-mini-2024-07-18\",\n",
    "  temperature     = 0.0\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f37f8c4-e741-4526-b6e7-0c91d0ec9242",
   "metadata": {},
   "outputs": [],
   "source": [
    "## How to make embedding using open-source Huggingface hub (https://huggingface.co)\n",
    "\n",
    "\"\"\" We will need to write the wrapper over Sentence Trancformer class \n",
    "due to implementation limitation inside langchain but if you are using it outside langchain it is not necessary \"\"\"\n",
    "\n",
    "class SentenceTransformerEmbeddings(Embeddings):\n",
    "    def __init__(self, model_name: str):\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "\n",
    "    def embed_documents(self, documents: List[str]) -> List[List[float]]:\n",
    "        return  [self.model.encode(d).tolist() for d in documents]\n",
    "\n",
    "    def embed_query(self, query: str) -> List[float]:\n",
    "        return self.model.encode([query])[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe1cc82-1057-4a35-9c4e-5652537af879",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'BAAI/bge-base-en-v1.5'\n",
    "embedding = SentenceTransformerEmbeddings(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07de3a44-1944-4749-bd97-bd668406be19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the documents into Chroma using the Azure OpenAI embeddings or the embedding from SentenceTransformers\n",
    "\n",
    "db = Chroma.from_documents(docs, embedding, collection_metadata = {\"hnsw:space\": \"cosine\"} )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6a51a4-e885-4b29-956c-232357f22aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you want to use different embeddings or different distance you need to delete the collection and reindex everything\n",
    "\n",
    "# db.delete_collection() #delete collection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c65cae8-ad97-4774-b356-fbb228ddc26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"what is the side effects of taking Delstrigo?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b02b05-0898-4a58-8dd6-00576e5a768d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Standard vanila similarity search \n",
    "\n",
    "retrieved_docs = db.similarity_search_with_score(query)\n",
    "retrieved_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88ff5d0-9c71-4e63-8327-104f79c2cfdb",
   "metadata": {},
   "source": [
    "### Hybrid search\n",
    "Hybrid search is a method that combines different search techniques(like sparse vectors, dense vectors, full-text search) to improve the accuracy and relevance of search results in information retrieval systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a34df5b-d166-4abc-9e03-8e52e50ea75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now lets perform vanila hybrid search. We will combine similarity with full-text search. \n",
    "# Vanila approach here is to use text query as a filter and show chunks that has the mentioned word inside the context\n",
    "\n",
    "docs_hybrid = db.similarity_search_with_score(query, k = 10, where_document = {\"$contains\": \"side effects\"})\n",
    "docs_hybrid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9468ed-04c8-4831-937d-5788e1ee48a3",
   "metadata": {},
   "source": [
    "### 3. Re-Rank Step (Optional)\n",
    "We will cover this in next lessons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67542a9-5de4-4c5e-95da-811ac35fb9aa",
   "metadata": {},
   "source": [
    "### 4. Result Representation\n",
    "Let's look how we can answer the question based on the results that we are getting in more user friendly output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac488c1-ac77-4bfd-860d-8a5ce0162996",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = AzureOpenAI(\n",
    "    api_key=os.environ['OPENAI_API_KEY'],\n",
    "    api_version=\"2023-07-01-preview\",\n",
    "    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n",
    "    azure_deployment='gpt-4-1106-preview'\n",
    ")\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    ")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model='gpt-4-1106-preview',\n",
    "    temperature=0,\n",
    "    messages=[\n",
    "      {\"role\": \"system\", \"content\": system_prompt},\n",
    "      {\"role\": \"user\", \"content\": f\"Query: {query} Docs: {retrieved_docs}\"}\n",
    "    ]\n",
    "  )\n",
    "\n",
    "print(f\"Took {time.time() - start} seconds to summarize documents with GPT-4.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a514a644ff9c121",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "response.choices[0].message.content"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b32d87a63d8db7c85c2cdcad4f6cc8b712ca746d91e154ea0cd737b1ef7d67c6"
  },
  "kernelspec": {
   "display_name": "Python 3.11.3 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
