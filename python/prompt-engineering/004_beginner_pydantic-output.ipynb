{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be0408d6",
   "metadata": {},
   "source": [
    "# Pydantic model for output format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7301a4ff",
   "metadata": {},
   "source": [
    "Author: Pavel Agurov, pavel_agurov@epam.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c00eadc",
   "metadata": {},
   "source": [
    "Pydantic allows to provide output instructions based on class model(s). To make our example fully \"class-based\" we will also create class for input data. But please note - you still should add {format_instructions} placeholder into prompt.\n",
    "\n",
    "The idea of this code is to compare 2 text list and build pairs. To have some explanation we will ask model to provide not only pairs, but score and explanation. It allows us to build not \"black box\" solution, but have some \"inside\" from model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636d84c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install openai > /dev/null\n",
    "%pip install tiktoken > /dev/null\n",
    "%pip install langchain > /dev/null\n",
    "%pip install langchain_openai > /dev/null\n",
    "%pip install langchain_core > /dev/null\n",
    "%pip install langchain_community > /dev/null\n",
    "%pip install langchain_text_splitters > /dev/null\n",
    "%pip install sentence-transformers > /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7324405",
   "metadata": {},
   "source": [
    "## Prompt with Pydantic model for output\n",
    "\n",
    "In this example we will build custom Pydantic model for output data and use it to provide instruction to the model and later to parse result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2900c99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "COMPARE_PROMPT_TEMPLATE = \"\"\"\n",
    "Your task is to find the best pairs between 2 string lists if possible.\n",
    "If you can't build pair for the item - just say \"no pair\".\n",
    "Be sure that you read all items from first list.\n",
    "Be sure that you check ALL items from second list and found the best fit.\n",
    "\n",
    "<first_list>\n",
    "{first_list}\n",
    "</first_list>\n",
    "\n",
    "<second_list>\n",
    "{second_list}\n",
    "</second_list>\n",
    "\n",
    "{format_instructions}\n",
    "\"\"\"\n",
    "\n",
    "@dataclass\n",
    "class InputStringList:\n",
    "    items : list[str]\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"\".join([f\"- {s}\\n\" for s in self.items])\n",
    "        \n",
    "\n",
    "class PairItem(BaseModel):\n",
    "    \"\"\"Pair item\"\"\"\n",
    "    first_item  : str   = Field(description=\"Item from the first list\")\n",
    "    second_item : str   = Field(description=\"Relevant item (if exists)\")\n",
    "    score       : float = Field(description=\"Score of relevance\")\n",
    "    explanation : str   = Field(description=\"Explain your decision\")\n",
    "\n",
    "class PairedList(BaseModel):\n",
    "    \"\"\"Pair list\"\"\"\n",
    "    pair_list : list[PairItem]\n",
    "        \n",
    "parser = PydanticOutputParser(pydantic_object= PairedList)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template= COMPARE_PROMPT_TEMPLATE,\n",
    "    input_variables=[\"first_list\", \"second_list\"],\n",
    "    partial_variables={\n",
    "        \"format_instructions\": parser.get_format_instructions()\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34296553",
   "metadata": {},
   "source": [
    "## LLM model\n",
    "\n",
    "Model should be powerful enough to be able to provide relevant result, but from another side - has reasonable price to make result profitable.\n",
    "\n",
    "Remember about temperature parameter - in langchain by default it's not 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "861ca4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "        api_key         = os.environ['OPENAI_API_KEY'],\n",
    "        api_version     = \"2023-07-01-preview\",\n",
    "        azure_endpoint  = \"https://ai-proxy.lab.epam.com\",\n",
    "        model           = \"gpt-4o-mini-2024-07-18\",\n",
    "        temperature     = 0.0\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00433207",
   "metadata": {},
   "source": [
    "## Chain\n",
    "\n",
    "You combile prompt, model and output parser into one chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73901dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | llm | parser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077e0e84",
   "metadata": {},
   "source": [
    "## Run\n",
    "\n",
    "- get_openai_callback here allows to have count of used tokens\n",
    "- function create_list creates list of string\n",
    "- call_llm will call LLM with invoke method and return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e22381b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.callbacks import get_openai_callback\n",
    "\n",
    "def call_llm(chain, first_list, second_list):\n",
    "    with get_openai_callback() as cb:\n",
    "        llm_result = chain.invoke({\n",
    "                \"first_list\"   : first_list, \n",
    "                \"second_list\"  : second_list\n",
    "        })\n",
    "        return llm_result, cb.total_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77c85a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_input_list  = InputStringList(\n",
    "    ['cat', 'dog', 'apple', 'computer']\n",
    ")\n",
    "second_input_list = InputStringList(\n",
    "    ['mouse', 'orange', 'shepherd']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ce2e357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used tokens: 602\n",
      "r.first_item='cat', r.second_item='mouse', r.score=0.5, r.explanation='Both are small animals'\n",
      "r.first_item='dog', r.second_item='shepherd', r.score=0.7, r.explanation='Shepherd is a breed of dog'\n",
      "r.first_item='apple', r.second_item='orange', r.score=0.6, r.explanation='Both are fruits'\n",
      "r.first_item='computer', r.second_item='no pair', r.score=0.0, r.explanation='No relevant item found in the second list'\n"
     ]
    }
   ],
   "source": [
    "object_result, tokens_used = call_llm(chain, first_input_list, second_input_list)\n",
    "print(f\"Used tokens: {tokens_used}\")\n",
    "\n",
    "# object_result is already PairedList\n",
    "for r in object_result.pair_list:\n",
    "    print(f\"{r.first_item=}, {r.second_item=}, {r.score=}, {r.explanation=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8988bca5",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "Please note - Pydantic model can't garantee you alwas correct output format. You will still have parsing exceptions in case of LLM hallucination. As you can see below it's still JSON format inside, just with additonal instructions. If JSON is wrong, you will have expection and error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "342f2333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Your task is to find the best pairs between 2 string lists if possible.\n",
      "If you can't build pair for the item - just say \"no pair\".\n",
      "Be sure that you read all items from first list.\n",
      "Be sure that you check ALL items from second list and found the best fit.\n",
      "\n",
      "<first_list>\n",
      "- cat\n",
      "- dog\n",
      "- apple\n",
      "- computer\n",
      "\n",
      "</first_list>\n",
      "\n",
      "<second_list>\n",
      "- mouse\n",
      "- orange\n",
      "- shepherd\n",
      "\n",
      "</second_list>\n",
      "\n",
      "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
      "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
      "\n",
      "Here is the output schema:\n",
      "```\n",
      "{\"description\": \"Pair list\", \"properties\": {\"pair_list\": {\"title\": \"Pair List\", \"type\": \"array\", \"items\": {\"$ref\": \"#/definitions/PairItem\"}}}, \"required\": [\"pair_list\"], \"definitions\": {\"PairItem\": {\"title\": \"PairItem\", \"description\": \"Pair item\", \"type\": \"object\", \"properties\": {\"first_item\": {\"title\": \"First Item\", \"description\": \"Item from the first list\", \"type\": \"string\"}, \"second_item\": {\"title\": \"Second Item\", \"description\": \"Relevant item (if exists)\", \"type\": \"string\"}, \"score\": {\"title\": \"Score\", \"description\": \"Score of relevance\", \"type\": \"number\"}, \"explanation\": {\"title\": \"Explanation\", \"description\": \"Explain your decision\", \"type\": \"string\"}}, \"required\": [\"first_item\", \"second_item\", \"score\", \"explanation\"]}}}\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "formatted_prompt = prompt.format(\n",
    "    first_list   = first_input_list, \n",
    "    second_list  = second_input_list\n",
    ")\n",
    "print(formatted_prompt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
