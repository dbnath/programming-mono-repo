{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be0408d6",
   "metadata": {},
   "source": [
    "# LLM Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a6c201",
   "metadata": {},
   "source": [
    "Author: Pavel Agurov, pavel_agurov@epam.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f59f83",
   "metadata": {},
   "source": [
    "When you work with ChatGpt, you can see that it supports conversation. You can ask a question and clarify details. This means that ChatGpt can remember the context of the conversation.\n",
    "But LLM only has only string prompt as input and has no memory. This means we have to store all the content into prompt and find a way to stay in prompt size limit.\n",
    "Let's see how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27bf7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install openai > /dev/null\n",
    "%pip install tiktoken > /dev/null\n",
    "%pip install langchain > /dev/null\n",
    "%pip install langchain_openai > /dev/null\n",
    "%pip install langchain_core > /dev/null\n",
    "%pip install langchain_community > /dev/null\n",
    "%pip install langchain_text_splitters > /dev/null\n",
    "%pip install sentence-transformers > /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5192ff03",
   "metadata": {},
   "source": [
    "Let's create connection to EPAM DIAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2900c99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "epam_dial = AzureChatOpenAI(\n",
    "        api_version     = \"2023-07-01-preview\",\n",
    "        azure_endpoint  = \"https://ai-proxy.lab.epam.com\",\n",
    "        model           = \"gpt-4o-mini-2024-07-18\",\n",
    "        temperature     = 0.0\n",
    "    ) \n",
    "\n",
    "\n",
    "query_chain  = epam_dial | StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29fe1c3e",
   "metadata": {},
   "source": [
    "And let's send simple query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b642aac9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello Pavel! How can I assist you today?'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_chain.invoke(\"Hello, my name is Pavel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e1a8bb",
   "metadata": {},
   "source": [
    "OK, not let's ask model about my name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "01b2b187",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm sorry, but I don't have access to personal information about individuals unless it has been shared with me in the course of our conversation.\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_chain.invoke(\"What is my name?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115880b1",
   "metadata": {},
   "source": [
    "As expected, model has no information about name."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27f6b83",
   "metadata": {},
   "source": [
    "## ConversationBufferWindowMemory\n",
    "\n",
    "Let's add memory. We will use ConversationBufferWindowMemory (https://python.langchain.com/v0.1/docs/modules/memory/types/buffer_window/). Parameter k defines how many interactions will be used. This can be useful for keeping a sliding window of the most recent interactions, so the buffer does not get too large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8299b965",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "memory = ConversationBufferWindowMemory(k = 10)\n",
    "conversation = ConversationChain(\n",
    "    llm    = epam_dial,\n",
    "    memory = memory,\n",
    "    verbose= False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "024c5447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Pavel! It's nice to meet you. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "response_object = conversation.invoke(\"Hello, my name is Pavel\")\n",
    "print(response_object[\"response\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "af3da3b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your name is Pavel.\n"
     ]
    }
   ],
   "source": [
    "response_object = conversation.invoke(\"What is my name?\")\n",
    "print(response_object[\"response\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a668441e",
   "metadata": {},
   "source": [
    "As you can see model remembers my first message and can response about name. Memory is stored inside ConversationBufferWindowMemory and provided into prompt. When model generates answer all memory will be return in answer object in field 'history'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "673062e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Hello, my name is Pavel\n",
      "AI: Hello Pavel! It's nice to meet you. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "print(response_object[\"history\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669383fb",
   "metadata": {},
   "source": [
    "We can see stored memory in conversation object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "60a748d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"Human: Hello, my name is Pavel\\nAI: Hello Pavel! It's nice to meet you. How can I assist you today?\\nHuman: What is my name?\\nAI: Your name is Pavel.\"}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6ff161",
   "metadata": {},
   "source": [
    "## ConversationSummaryMemory\n",
    "\n",
    "There are more complex memory type - ConversationSummaryMemory (https://python.langchain.com/v0.1/docs/modules/memory/types/summary/): \n",
    "*\"This type of memory creates a summary of the conversation over time. This can be useful for condensing information from the conversation over time. Conversation summary memory summarizes the conversation as it happens and stores the current summary in memory. This memory can then be used to inject the summary of the conversation so far into a prompt/chain. This memory is most useful for longer conversations, where keeping the past message history in the prompt verbatim would take up too many tokens.\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "37a223c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationSummaryMemory\n",
    "\n",
    "# memory uses LLM to build summary\n",
    "memory = ConversationSummaryMemory(llm = epam_dial)\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    llm    = epam_dial,\n",
    "    memory = memory,\n",
    "    verbose= False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0390e2ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Pavel! It's nice to meet you. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "response_object = conversation.invoke(\"Hello, my name is Pavel\")\n",
    "print(response_object[\"response\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9c590ac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Bob! No problem at all. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "response_object = conversation.invoke(\"Oh, sorry, I was wrong, my name is Bob\")\n",
    "print(response_object[\"response\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fa4d694b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your name is Bob.\n"
     ]
    }
   ],
   "source": [
    "response_object = conversation.invoke(\"What is my name?\")\n",
    "print(response_object[\"response\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ac744c",
   "metadata": {},
   "source": [
    "But let's see now what we have in the memory - it's not just our dialog, it's summary of all our conversations. But please be aware that we can lost important details during summarization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7e673d",
   "metadata": {},
   "source": [
    "## Save and load memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33314df3",
   "metadata": {},
   "source": [
    "We can get memory as json and save it into file. In fact, it will save all context of our conversation and we can easy restore it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "42e5f8cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'The human introduces themselves as Pavel, but corrects themselves and says their name is Bob. The AI greets Bob and asks how it can assist him. When Bob asks what his name is, the AI confirms that his name is indeed Bob.'}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be1f65e",
   "metadata": {},
   "source": [
    "We can upload some information into memory in advance (from file or db)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ab311253",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationSummaryMemory, ChatMessageHistory\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationSummaryMemory\n",
    "\n",
    "history = ChatMessageHistory()\n",
    "history.add_user_message(\"My name is Pavel\")\n",
    "history.add_user_message(\"My favorite color is blue\")\n",
    "\n",
    "memory = ConversationSummaryMemory.from_messages(\n",
    "    llm         = epam_dial,\n",
    "    chat_memory = history\n",
    ")\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    llm    = epam_dial,\n",
    "    memory = memory,\n",
    "    verbose= False\n",
    ")\n",
    "\n",
    "# we can also add memory directly\n",
    "# conversation.memory.save_context({\"input\": \"hi\"}, {\"output\": \"whats up\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0eaf4b1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your name is Pavel, and you mentioned that your favorite color is blue.\n"
     ]
    }
   ],
   "source": [
    "response_object = conversation.invoke(\"What is my name and what color is the best?\")\n",
    "print(response_object[\"response\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db309c7",
   "metadata": {},
   "source": [
    "## Other memory types\n",
    "\n",
    "**ConversationSummaryBufferMemory** (https://python.langchain.com/v0.1/docs/modules/memory/types/summary_buffer/) combines the two ideas. It keeps a buffer of recent interactions in memory, but rather than just completely flushing old interactions it compiles them into a summary and uses both. It uses token length rather than number of interactions to determine when to flush interactions.\n",
    "\n",
    "**ConversationTokenBufferMemory** (https://python.langchain.com/v0.1/docs/modules/memory/types/token_buffer/) keeps a buffer of recent interactions in memory, and uses token length rather than number of interactions to determine when to flush interactions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
