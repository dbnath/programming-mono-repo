{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be0408d6",
   "metadata": {},
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a820f300",
   "metadata": {},
   "source": [
    "Author: Pavel Agurov, pavel_agurov@epam.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427ee342",
   "metadata": {},
   "source": [
    "## 001_beginner_first_meet\n",
    "\n",
    "This notebook is designed as an easy-to-follow guide for those new to working with Large Language Models (LLMs). It provides step-by-step instructions on setting up and using LLMs through platforms like OpenAI, Azure OpenAI, and EPAM DIAL. Starting with the basics of configuring API keys and making simple API calls, the notebook explains how to interact with LLMs, parse their responses, and manage conversation history. It introduces beginner-friendly tools like OpenAI SDK, Azure SDK, and LangChain to simplify complex tasks. With practical examples and clear explanations, it helps users understand token usage and model selection, making it an ideal resource for anyone starting their journey into LLM development."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18db849c",
   "metadata": {},
   "source": [
    "## 001_beginner_simple-first-connection\n",
    "\n",
    "This notebook is a step-by-step guide on how to connect to a Large Language Model (LLM) using different platforms. These platforms include OpenAI, Azure, and EPAM DIAL. It starts by showing how to install the necessary software. Then, it explains how to connect to each platform. For OpenAI, you need an OPENAI_API_KEY. For Azure, you need more details like the AZURE_OPENAI_API_KEY, API version, and others. For EPAM DIAL, you need a DIAL KEY from the EPAM chat lab. The notebook also teaches how to run a simple question or 'prompt' on these platforms. It shows how to make sense of the answer from the LLM. It also explains how to create a question with specific details and how to print the result. Lastly, it shows how to count how many 'tokens' or pieces of information were used during an LLM call. This is important to keep track of how much information is being used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff8148a",
   "metadata": {},
   "source": [
    "## 002_beginner_embedding\n",
    "\n",
    "This notebook delves into the workings of language models, focusing on the conversion of text into numeric vectors using an 'embedding model'. It explains how these models create a numeric vector, or 'embedding\", for each word, aiming to generate similar vectors for words with similar meanings. The importance of context in determining word similarity is emphasized, illustrated with examples. The notebook further explores the use of different embedding models, such as SBERT and Ada, demonstrating how to generate and compare word vectors. It also introduces the concept of tokens, which can represent not just words or parts of words, but also punctuation or special characters. The limitations of these models, such as their inability to compare meanings or understand context beyond their training data, are also discussed. Practical tips for efficient model usage, including offline mode and caching for embeddings, are provided."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1d424a",
   "metadata": {},
   "source": [
    "## 003_beginner_simple-formatted-ouput\n",
    "\n",
    "This notebook showcases how to use a language model to generate output in structured formats like XML or JSON. The model is tasked with comparing two lists of text strings and creating pairs. For each pair, the model also provides a score indicating the relevance of the pair and an explanation for its decision. This approach allows for a more transparent solution rather than a 'black box' model. The notebook also includes code for handling potential issues with the output format, such as JSON corruption during long output. It provides functions to fix common JSON issues and to clean up text to avoid problems with JSON parsing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa443484",
   "metadata": {},
   "source": [
    "## 004_beginner_pydantic-output\n",
    "\n",
    "This notebook provides an example of using a language model to compare two lists of strings and generate pairs. Each pair includes a relevance score and an explanation. The notebook uses Pydantic, a data validation and settings management library, to define the data models for the input string lists and the output pairs. The Pydantic models are then used to parse the output from the language model. The notebook also demonstrates how to use a PromptTemplate to define the prompt for the language model, including the instructions for the output format based on the Pydantic models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1fe088",
   "metadata": {},
   "source": [
    "## 005_beginner_cache\n",
    "\n",
    "This notebook illustrates the use of a cache when working with a language model. The cache can be a cost-effective and efficient tool when running multiple queries, especially when only one query changes. For instance, during development, you can run a query once, store it in the cache, and use it repeatedly until a new query is needed. This approach is faster and cheaper than running the query each time. The notebook uses SQLite for caching and demonstrates the time saved when a cached query is run."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543e3293",
   "metadata": {},
   "source": [
    "## 006_beginner_llm_memory\n",
    "\n",
    "This notebook explores how to use memory with a language model to maintain the context of a conversation. While the language model itself does not have memory and only takes a string prompt as input, the notebook shows how to store all the content into the prompt and manage the prompt size limit. It introduces the use of ConversationBufferWindowMemory and ConversationSummaryMemory to store recent interactions or summarize the conversation over time. The notebook also demonstrates how to save and load memory, allowing the context of a conversation to be easily restored later.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9002d8e6",
   "metadata": {},
   "source": [
    "## 007_beginner_document_loaders\n",
    "\n",
    "The notebook provides a guide on how to extract plain text from various data sources like PDFs and HTML using different loaders and parsers such as UnstructuredPDFLoader, UnstructuredFileLoader, OnlinePDFLoader, PyPDFLoader, and Beautiful Soup. This process is crucial, for example, for the Retriever-Augmented Generation (RAG) as it relies on clean, plain text data for efficient operation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7ace4d",
   "metadata": {},
   "source": [
    "## 008_chunking\n",
    "\n",
    "The notebook discusses the concept of text chunking, which involves splitting a large text into smaller parts or \"chunks\" to improve the efficiency of LLMs. The notebook demonstrates how to split text using different strategies such as fixed size chunking, sentence-based chunking, content-based chunking, and semantic-based chunking. It also discusses the challenges of splitting text, such as the potential loss of information and issues with headers and pronouns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a78106d",
   "metadata": {},
   "source": [
    "## 009_beginner_vectorDB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc0e899",
   "metadata": {},
   "source": [
    "The notebook discusses the concept of vector databases and similarity search, which are used to find relevant chunks of text in large datasets.\n",
    "\n",
    "The notebook proceeds to demonstrate the use of Facebook's FAISS library for similarity search. It shows how to create a vector store from the chunks, save it to disk, and load it back. It also demonstrates how to perform a similarity search with a score threshold and a filter function.\n",
    "\n",
    "The notebook then introduces Qdrant, another vector database, and demonstrates how to use it for similarity search. It shows how to create a vector store, perform a similarity search with a score threshold, and define a filter as a dictionary or a custom filter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c2c73e",
   "metadata": {},
   "source": [
    "## 010_beginner_RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd76ca35",
   "metadata": {},
   "source": [
    "TBD"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
